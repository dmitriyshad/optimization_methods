{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фабарисов Дмитрий. Выпуклый анализ и оптимизация. Задание 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cvxopt as cp\n",
    "cp.solvers.options['show_progress'] = False\n",
    "\n",
    "from scipy import optimize as opt\n",
    "import copy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим вспомогательные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac {\\partial f} {\\partial x_1} = \\frac {x_1 - 1} {2} + 8 x_1 (2 x_1^2 - x_2 - 1)$$\n",
    "\n",
    "$$\\frac {\\partial f} {\\partial x_n} = - 2(2 x_{n-1}^2 - x_n - 1)$$\n",
    "\n",
    "$$\\frac {\\partial f} {\\partial x_i} = 8 x_i (2 x_i^2 - x_{i+1} - 1) - 2(2 x_{i-1}^2 - x_i - 1),\\ \\ \\ \\ i = 2, ..., n - 1$$\n",
    "\n",
    "$$f'' (x) =\n",
    "\\begin{pmatrix}\n",
    "48 x_1^2 - 8 x_2 - 7.5 & -8 x_1 & 0 & ... & 0 & 0\\\\\n",
    "-8 x_1 & 48 x_2^2 - 8 x_3 - 6 & -8 x_2 & ... & 0 & 0\\\\\n",
    "0 & -8 x_2 & 48 x_3^2 - 8 x_4 - 6 & ... & 0 & 0\\\\\n",
    "... & & & & & \\\\\n",
    "0 & 0 & 0 & ... & 48 x_{n-1}^2 - 8 x_n - 6 & -8 x_{n-1} \\\\\n",
    "0 & 0 & 0 & ... & -8 x_{n-1} & 2 \\\\\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_difference(func, grad_func, f_true, x, x_true):\n",
    "    delta_x = np.linalg.norm(x - x_true)\n",
    "    delta_func = np.abs(func(x) - f_true)\n",
    "    abs_grad = np.linalg.norm(grad_func(x))\n",
    "    \n",
    "    return delta_x, delta_func, abs_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    l = len(x)\n",
    "    res = 0\n",
    "    res += (x[0] - 1)**2 / 4\n",
    "    for i in range(1, l):\n",
    "        res += (2 * x[i-1]**2 - x[i] - 1) ** 2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_func(x):\n",
    "    l = len(x)\n",
    "    grad = np.zeros(l)\n",
    "    grad[0] = 16*x[0]**3 - 8*x[0]*x[1] - 7.5*x[0] - 0.5\n",
    "    grad[l-1] = -4*x[l-2]**2 + 2*x[l-1] + 2\n",
    "    for i in range(1, l-1):\n",
    "        grad[i] = 16*x[i]**3 - 4*x[i-1]**2 - 8*x[i]*x[i+1] - 6*x[i] + 2\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hess_func(x):\n",
    "    n = len(x)\n",
    "    hess = np.zeros([n, n])\n",
    "    hess[0,0] = 48*x[0]**2 - 8*x[1] - 7.5\n",
    "    hess[0,1] = -8*x[0]\n",
    "    hess[1,0] = hess[0,1]\n",
    "    hess[n-1, n-1] = 2\n",
    "    hess[n-1, n-2] = -8*x[n-2]\n",
    "    hess[n-2, n-1] = hess[n-1, n-2]\n",
    "    for i in range(1, n-1):\n",
    "        hess[i, i] = 48*x[i]**2 - 8*x[i+1] - 6\n",
    "        \n",
    "        hess[i, i-1] = -8*x[i-1]\n",
    "        hess[i, i+1] = -8*x[i+1]\n",
    "        \n",
    "        hess[i-1, i] = hess[i, i-1]\n",
    "        hess[i+1, i] = hess[i, i+1]\n",
    "    \n",
    "    return np.matrix(hess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задаем размерность, генерируем $x$, задаем истинные значения минимума $x_{true}$, $y_{true}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimension = 3\n",
    "x = [1.5] + [1.0]*(dimension - 1)\n",
    "\n",
    "x_true = [1.0] * dimension\n",
    "f_true = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x0, func, grad_func, check_difference, step, tol, max_steps=100000):\n",
    "    x = np.array(x0)\n",
    "    i = 0\n",
    "    while True:\n",
    "\n",
    "        stats = check_difference(func, grad_func, f_true, x, x_true)\n",
    "        if (i > max_steps):\n",
    "            return stats\n",
    "        if (stats[2] < tol):\n",
    "            return x, i, stats[2], stats[0], stats[1] \n",
    "        \n",
    "        x -= step * grad_func(x)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print gradient_descent(x, func, grad_func, check_difference, 0.01, 10 ** (-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод сопряженных градиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conjugate_gradient(x0, func, grad_func, check_difference, tol, max_steps=10000):\n",
    "    x = np.array(x0)\n",
    "    i = 0\n",
    "    S = -grad_func(x)\n",
    "    new_grad = -S\n",
    "    while True:\n",
    "        i += 1\n",
    "        stats = check_difference(func, grad_func, f_true, x, x_true)\n",
    "        \n",
    "        if (i > max_steps):\n",
    "            return \"too much steps\", x, i, stats\n",
    "        if (stats[2] < tol):\n",
    "            return x, i, stats[2], stats[0], stats[1] \n",
    "        def func_lambd(lambd):\n",
    "            return func(x + lambd * S)\n",
    "        prev_grad = new_grad\n",
    "        lambd = opt.minimize(func_lambd, 0.0, method='Powell').x\n",
    "        x += lambd*S\n",
    "        new_grad = grad_func(x)\n",
    "        w = np.linalg.norm(new_grad)**2 / np.linalg.norm(prev_grad)**2\n",
    "        S = -new_grad + w*S\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print conjugate_gradient(x, func, grad_func, check_difference, 10 ** (-9), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод наискорейшего спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimal_gradient_descent(x0, func, grad_func, check_difference, tol, max_steps=10000):\n",
    "    x = np.array(x0)\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        stats = check_difference(func, grad_func, f_true, x, x_true)\n",
    "        if (i > max_steps):\n",
    "            return \"too much steps\", x, i, stats\n",
    "        if (stats[2] < tol):\n",
    "            return x, i, stats[2], stats[0], stats[1]\n",
    "        \n",
    "        S = -grad_func(x)\n",
    "        def func_lambd(lambd):\n",
    "            return func(x + lambd * S)\n",
    "        lambd = opt.minimize(func_lambd, 0.0, method='Powell').x\n",
    "        x += lambd*S\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print optimal_gradient_descent(x, func, grad_func, check_difference, 10 ** (-8), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton_method(x0, func, grad_func, hess_func, check_difference, tol, max_steps=100):\n",
    "    x = np.array(x0)\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        stats = check_difference(func, grad_func, f_true, x, x_true)\n",
    "        if (i > max_steps):\n",
    "            return \"too much steps\", x, i, stats\n",
    "        if (stats[2] < tol):\n",
    "            return x, i, stats[2], stats[0], stats[1]\n",
    "        micro_step = 0.001\n",
    "        S = grad_func(x)\n",
    "        H = hess_func(x)\n",
    "        res = H.I * np.matrix(S).T\n",
    "        x -= np.asarray(res).ravel()\n",
    "        if (i % 2000 == 0):\n",
    "            print i, x, func(x), numpy.linalg.det(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 [  1.40756317   2.98056518  16.76829658] 0.0418550128114 -1111006.95438\n",
      "4000 [  1.41236245   3.00778326  17.09427859] 0.0428442576095 -1163208.95635\n",
      "6000 [  1.4170423    3.03441179  17.4160676 ] 0.0438199846534 -1216120.26958\n",
      "8000 [  1.421609     3.06048004  17.73383317] 0.0447826839912 -1269726.86242\n",
      "10000 [  1.42606833   3.086015    18.0477335 ] 0.0457328172357 -1324015.36506\n",
      "12000 [  1.43042563   3.11104169  18.3579165 ] 0.046670819786 -1378973.02101\n",
      "14000 [  1.43468583   3.13558334  18.66452068] 0.0475971028346 -1434587.6434\n",
      "16000 [  1.43885352   3.15966152  18.96767603] 0.0485120551863 -1490847.57545\n",
      "18000 [  1.44293293   3.18329635  19.26750467] 0.0494160449071 -1547741.65456\n",
      "20000 [  1.44692805   3.20650659  19.56412158] 0.050309420823 -1605259.17969\n",
      "22000 [  1.45084255   3.22930978  19.85763512] 0.0511925138851 -1663389.88166\n",
      "24000 [  1.45467989   3.25172236  20.14814758] 0.0520656384132 -1722123.89596\n",
      "26000 [  1.45844329   3.27375973  20.43575565] 0.0529290932323 -1781451.73791\n",
      "28000 [  1.46213577   3.29543636  20.72055085] 0.0537831627124 -1841364.2798\n",
      "30000 [  1.46576019   3.31676585  21.00261989] 0.0546281177209 -1901852.72997\n",
      "32000 [  1.46931919   3.33776103  21.28204503] 0.0554642164968 -1962908.61346\n",
      "34000 [  1.47281528   3.35843398  21.55890442] 0.0562917054537 -2024523.7542\n",
      "36000 [  1.47625084   3.37879612  21.83327234] 0.0571108199196 -2086690.25852\n",
      "38000 [  1.47962809   3.39885822  22.1052195 ] 0.0579217848181 -2149400.49994\n",
      "40000 [  1.48294913   3.4186305   22.37481328] 0.058724815297 -2212647.10497\n",
      "42000 [  1.48621596   3.43812264  22.64211791] 0.0595201173101 -2276422.94004\n",
      "44000 [  1.48943045   3.45734379  22.9071947 ] 0.0603078881544 -2340721.09923\n",
      "46000 [  1.4925944    3.47630267  23.17010225] 0.0610883169684 -2405534.89294\n",
      "48000 [  1.49570949   3.49500756  23.43089656] 0.0618615851946 -2470857.83724\n",
      "50000 [  1.49877733   3.51346633  23.68963123] 0.0626278670084 -2536683.64402\n",
      "52000 [  1.50179945   3.53168646  23.94635759] 0.0633873297173 -2603006.21165\n",
      "54000 [  1.50477731   3.54967509  24.20112483] 0.0641401341322 -2669819.61637\n",
      "56000 [  1.50771227   3.56743905  24.45398014] 0.0648864349139 -2737118.10414\n",
      "58000 [  1.51060566   3.58498481  24.70496881] 0.0656263808956 -2804896.08303\n",
      "60000 [  1.51345874   3.6023186   24.95413432] 0.0663601153845 -2873148.11601\n",
      "62000 [  1.51627271   3.61944634  25.20151849] 0.0670877764439 -2941868.91429\n",
      "64000 [  1.51904869   3.6363737   25.4471615 ] 0.0678094971564 -3011053.33092\n",
      "66000 [  1.5217878    3.65310613  25.69110205] 0.0685254058711 -3080696.35484\n",
      "68000 [  1.52449108   3.66964882  25.93337737] 0.069235626435 -3150793.10525\n",
      "70000 [  1.52715952   3.68600676  26.17402337] 0.0699402784103 -3221338.82629\n",
      "72000 [  1.52979409   3.70218475  26.41307462] 0.070639477278 -3292328.882\n",
      "74000 [  1.5323957    3.71818736  26.6505645 ] 0.0713333346298 -3363758.75162\n",
      "76000 [  1.53496524   3.73401901  26.88652519] 0.0720219583482 -3435624.02503\n",
      "78000 [  1.53750354   3.74968395  27.12098779] 0.0727054527762 -3507920.39855\n",
      "80000 [  1.54001142   3.76518623  27.35398233] 0.0733839188773 -3580643.67085\n",
      "82000 [  1.54248966   3.78052979  27.58553783] 0.0740574543861 -3653789.73918\n",
      "84000 [  1.54493901   3.7957184   27.81568234] 0.0747261539504 -3727354.59568\n",
      "86000 [  1.54736018   3.81075569  28.04444302] 0.0753901092657 -3801334.32397\n",
      "88000 [  1.54975387   3.82564516  28.27184613] 0.0760494092019 -3875725.09583\n",
      "90000 [  1.55212075   3.84039018  28.49791712] 0.0767041399234 -3950523.16813\n",
      "92000 [  1.55446146   3.85499402  28.72268061] 0.0773543850024 -4025724.87979\n",
      "94000 [  1.55677662   3.86945981  28.9461605 ] 0.0780002255266 -4101326.64899\n",
      "96000 [  1.55906681   3.88379058  29.16837992] 0.0786417402009 -4177324.97048\n",
      "98000 [  1.56133263   3.89798927  29.38936132] 0.0792790054439 -4253716.41297\n",
      "100000 [  1.56357462   3.91205871  29.60912647] 0.0799120954796 -4330497.61668\n",
      "('too much steps', array([  1.56357462,   3.91205871,  29.60912647]), 100001, (28.762472417297033, 0.07991209547956292, 0.022573398469676521))\n"
     ]
    }
   ],
   "source": [
    "print newton_method(x, func, grad_func, hess_func, check_difference, 10 ** (-2), 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
